\chapter{A Smartphone-based Tool for Parkinsonian Hand Tremor Assessment}
\label{ch:smartphone}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[OC]{\leftmark}
\fancyhead[EC]{\rightmark}
\cfoot{\thepage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{General Approach}
\label{sec:smartGenApproach}
As discussed in section ~\ref{subsec:smartphones}, smartphones are becoming useful platforms in the hands of researchers, clinicians and medical professionals, who by developing sophisticated software and protocols can incorporate these devices into various stages of clinical practice. Even cheap devices are equipped with embedded inertial sensors, putting accelerometers and gyroscopes in the hands of every user. Combined with limitless connectivity and substantial processing power, they have the potential to become powerful quantification and monitoring tools.

Incentivized by the research discussed in ~\ref{subsec:smartphones}, including the work of LeMoyne (2010) and Deneault (2013), we set out to investigate how the sensors embedded in a smartphone would perform in a clinical setting or even home environment, and if they could provide any useful methods for \gls{PD} symptom quantification, identification and monitoring. Our approach was to use a popular smartphone device (iPhone) and incorporate it in the same setting as the normal scale-based examination, namely \gls{UPDRS} scoring, in a non-invasive, tireless manner. Having the six axes of innovation in mind (see ~\ref{sec:axes}), our objectives revolved around cost-efficiency, availability, ease of use, quality of diagnosis, effective monitoring and biobanking. 

Our approach was based on the following pillars: 

\begin{enumerate}
\item We focused on exploring the use of a ubiquitous device, such as a smartphone, with no additional software installed apart from its factory-installed operating system, and without the need to attach external hardware on it. 
\item We avoided making assumptions about the users, i.e. that they would have e-mail accounts or that they would be proficient enough to install and manage applications and configure complex settings. 
\item We explored solutions that would not require the presence of an expert to use, but would still produce replicable and accurate symptoms quantification. 
\item We opted for cloud based approaches that would ensure the persistence of the data and the availability of the results for post-processing and verification. 
\end{enumerate}

\noindent
To validate our symptoms quantification approach we conducted two separate clinical trials: 

\begin{enumerate}
\item The first was a small-scale case control pilot clinical trial (\gls{SmartCT1}), which would serve as proof-of-concept for our approach (Kostikis et al, 2011). It consisted of 10 patients with idiopathic \gls{PD}, defined as group \gls{SmartPD1}, and 10 age-matched healthy volunteers, defined as group \gls{SmartH1}. The data collected were post-processed using signal processing and statistically analyzed, to calculate quantifying metrics and establish their significance. 
\item The second was a larger case control clinical trial (\gls{SmartCT2}), were 23 patients with idiopathic \gls{PD}, defined as group \gls{SmartPD2}, and 20 age-matched healthy volunteers, defined as group \gls{SmartH2}, were recruited. In a preliminary validation study, we used the data collected from \gls{SmartPD2} to compare the quantification calculated by our tool, to the patients' \gls{UPDRS} scores (Kostikis et al, 2014). Later on, both the \gls{SmartPD2} and \gls{SmartH2} groups' data were processed and used to perform statistical analysis and build machine learning models to establish our tool's potential as a classification platform for \gls{PD} patients (Kostikis et al, 2015). During the second clinical trial we also conducted a small longitudinal trial (\gls{SmartCT2L}) recruiting two idiopathic \gls{PD} patients, defined as \gls{SmartPD2L}. They were inpatients and were screened with our tool twice, once before, and once after medication. 
\end{enumerate}

In the following section we will describe the smartphone-based clinical trials we conducted, present the data processing pipeline applied in each trial and discuss the results obtained in each case, and the implications derived from our smartphone-based \gls{PD} symptoms quantification approach. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{SmartCT1}
\label{sec:SmartCT1}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Protocol}
\label{subsec:SmartCT1Protocol}

\subsubsection{Rationale}
\label{subsubsec:smartCT1Rationale}
In section \ref{sec:accelerometry} we reviewed the literature proving that accelerometers are great means of studying human motor behavior and quantifying symptoms of movement disorders. In paragraph \ref{subsec:smartphones} we reviewed the papers that proved smartphones featuring Inertial measurement units (\gls{IMU}) sensors could be considered viable options for quantifying and measuring \gls{PD} motor symptoms.

Although accelerometers themselves and accelerometry-enabled smartphones had been proven to work, the rationale of this clinical trial was to test the accuracy of a simple web-based tool in distinguishing healthy individuals from \gls{PD} patients, by assessing upper extremity action postural tremor (see section \ref{subsec:tremor}) for a few seconds. Up until the inception of the trial's context by our team (late 2010), resources such as \gls{IMU} sensors were only available in a mobile platform like Apple's iOS\footnote{Apple iOS, accessed 02/10/2017 at: \url{https://www.apple.com/lae/ios/}} and Google's Android\footnote{Android, accessed 02/10/2017 at: \url{https://www.android.com/}}, through native applications, developed specifically for each particular target platform. Such an application would have to be downloaded and installed to properly function. Our approach was to harness a recently available (at the time) JavaScript specification, supported in Apple's iOS 4.2 Safari browser\footnote{DeviceOrientation event specification, accessed 01/10/2017 at: \url{http://dev.w3.org/geo/api/spec-source-orientation.html}}. Opting for a web-based solution meant that the user could execute our protocol without being required to install any application or even own the mobile device. A web-based solution built on a widely adopted specification would also mean that our tool could be compatible ``out-of-the-box'' with a vast range of devices, with varying physical characteristics and operating systems. Finally, the web-based solution allowed us to seamlessly upload the collected signal upon completion of the recording, absolving the user from the responsibility to handle the data and send them via e-mail, or be involved in any other way in the signal collection and transmission. 

Another novelty at the time (late 2010) was the combination of gyroscope as well as accelerometer signals. We were specifically interested in obtaining both acceleration and rotational velocity data because observations of patients with movement disorders show that tremor in their upper extremities may have a significant rotational component.

We chose to conduct a control study and not a one-population validation study because there was complete lack of literature evidence that this method of collecting inertial signals would work, or even be robust enough for us to gather usable data, and detection of a motor anomaly should be the first step before attempting to evaluate its severity. Our primary goal was to establish whether our tool could identify tremor. In a follow-up clinical trial we would assess its accuracy in quantifying the symptom (see section \ref{sec:SmartCT2}). 

Therefore, the expected outcome of \gls{SmartCT1} was to collect data using a novel tool on a small target population (\gls{SmartPD1}), examine the signals, calculate basic metrics and explore the statistical significance of the metrics when compared to a size-matched control population (\gls{SmartH1}). Most importantly, we wanted to gain significant insight concerning the optimization of the protocol of our next scheduled larger clinical trial (\gls{SmartCT2}).

\subsubsection{Volunteers}
\label{subsubsec:smartCT1Volunteers}

To conduct \gls{SmartCT1} we recruited 20 volunteers. Ten of them were healthy control subjects (\gls{SmartH1}) and 10 patients (\gls{SmartPD1}). The latter population was recruited from the outpatient clinic of the 1st Department of Neurology of AHEPA University General Hospital, located in the territory of the Aristotle University of Thessaloniki. All subjects agreed to participate in this research after a detailed explanation of its aims and of the testing procedure. Eight out of 10 volunteers of the \gls{SmartPD1} group were \gls{PD} patients. One suffered from cerebellar tremor and one had psychogenic parkinsonism. All \gls{SmartPD1} volunteers were under medication for the alleviation of their symptoms.

The inclusion criteria for the \gls{SmartH1} group was the absence of any kind of parkinsonism, whether idiopathic, atypical or secondary (see \ref{sec:parkinsonism}), both regarding themselves and their close relatives. The \gls{SmartH1} were not age-matched because our main goal was to compare pathological signals against healthy ones, and not explore the correlation between the signals collected and the patients' status. 

(\textcolor{BurntOrange}{Table X Demographics of the CT1 population}).

\subsubsection{Procedure and Hardware}
\label{subsubsec:smartCT1ProcHardware}

Although our metrics and results from \gls{SmartCT1} were not to be compared with the clinical examination mainstay, the \gls{UPDRS}, we applied the same instructions regarding body posture as described in item 3.15 - Postural tremor of the hands of the \gls{MDS}-\gls{UPDRS} (Goetz et al, 2008). More specifically, in order to assess postural hand tremor we instructed the volunteers to stretch their arms out in front of the body, keeping their palms facing the ground. The wrists were to be held straight and the fingers separated as to not touch each other, in a comfortable manner. This position was to be held for 12 seconds. Although the position is kept for 10 seconds in the directive of the \gls{UPDRS}, we opted for two more seconds to allow for errors recorded before and after each session. 

The position described above was performed while the volunteers were wearing a custom-made glove with an iPhone securely attached on it. Internet access had to be enabled on the phone, and screen orientation had to be ``locked'' to avoid mislabeling the data. We wanted to avoid having to compensate for unintentional movements, not related to the \gls{PD} symptoms, therefore we chose to keep the iPhone securely attached to the volunteers hands via a glove, and not have them hold it in their hand or place it loosely on their limbs. Having the participants hold the device would result in voluntary muscle activation, undermining the definition of postural tremor. There are studies were the patient assumes a position similar to the one described above and in the \gls{MDS}-\gls{UPDRS} item 3.15, but having the patients hold the device in their hands (Kassavetis et al, 2015). This, while efficient in the context of symptoms' quantification, ultimately cannot be comparable to results from a \gls{UPDRS} examination. This was not acceptable in our setting, where validation of our tool would be a desired outcome, requiring correlation analysis with the clinical examination ``golden'' standard, the \gls{UPDRS} at later stages of our research. 

In order to keep the presence of the smartphone as discreet as possible we built an inexpensive apparatus consisting of an elastic glove, a perforated, rigid smartphone shell case and some non-elastic thread. The result was an easy to put on and lightweight glove-case that would keep the phone attached on the patients' dorsal part of the hand, without hindering the position to be assumed, according to the procedure described earlier. Unfortunately the weight of the device at 140 grams was not trivial and could potentially slightly dampen the symptoms, but at least it was the same for all participants across our smartphone-based clinical trials and it was not something we could eradicate. 

(\textcolor{BurntOrange}{Figure consisting of three parts a. the glove, b. the case, c. some thread and d. the final custom apparatus}).

\begin{figure}[h]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=0.7\linewidth]{content/images/myHand/hand}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=0.7\linewidth]{content/images/myHand/figure_1_Color}
\end{subfigure}
\caption{The glove-case worn on hand with a device attached}
\label{fig:glovCase}
\end{figure}

The smartphone itself used in \gls{SmartCT1} was an Apple iPhone 4 GSM\footnote{iphone 4 technical specifications, accessed 01/10/2017 at: \url{https://support.apple.com/kb/sp587}}, powered by iOS version 4.2\footnote{iOS 4.2 software update, accessed 01/10/2017 at: \url{https://support.apple.com/kb/dl1061}}. 

\subsubsection{Software}
\label{subsubsec:smartCT1Software}
The software required for \gls{SmartCT1} consisted of the following components:

\begin{enumerate}
\item A custom made web application, built with PHP, HTML and JavaScript.
\item An Apache\footnote{Apache http server project, accessed 02/10/2017 at: \url{https://httpd.apache.org/}} web server which hosted the web application and allowed it to store simple text files.
\item A data processing pipeline written in MATLAB\footnote{Matlab, accessed 02/10/2017 at: \url{https://www.mathworks.com/products/matlab.html}} (various versions).
\end{enumerate}

The web application was designed to be as simple as possible, consisting of two views. The landing page was a simple view containing a form where the clinical trial associate or the support staff would fill in an anonymized unique identifier for the patient (Figure \ref{fig:webAppForm1}), details about the posture assumed for every recording session, which for \gls{SmartCT1} was set by default to ``Extended'' (Figure \ref{fig:webAppForm2}), and left or right hand (Figure \ref{fig:webAppForm3}). All but one the controls of the form would be hidden at first and revealed gradually to enforce a step-by-step data entry procedure, discouraging the submission of partially filled forms (Figure \ref{fig:webAppForm}). The button submitting the form would be the last one to be displayed, allowing the initiation of the collection. At that point the volunteer should already have resumed the required posture for the signal recording. 

\begin{figure}[h]
\centering
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{content/images/screens/screenA1}
  \caption{Filling in the identifier}
  \label{fig:webAppForm1}
\end{subfigure}%
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{content/images/screens/screenA2}
  \caption{Filling in the posture}
  \label{fig:webAppForm2}
\end{subfigure}
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{content/images/screens/screenA3}
  \caption{Filling in the side}
  \label{fig:webAppForm3}
\end{subfigure}
\caption{The web application form progressive screens}
\label{fig:webAppForm}
\end{figure}

When the clinical trial associate or the support staff launched the recording, by submitting the form of the first view, the screen of the iPhone would turn red for 3 seconds (Figure \ref{fig:webApp1}) and then automatically green, showing that the collection of the signals generated by the \gls{IMU} sensors was activated (Figure \ref{fig:webApp2}). After 12 seconds (resulting in a total of 15 from the form submission), the data collected from the sensors would automatically be posted to the Apache server in the form of a text file. The text files were then gathered and handled by a series of scripts built in MATLAB, in order to be cleaned, sorted and processed to extract the metrics and study the results, as will be explained later in this manuscript. 

\begin{figure}[h]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.7\linewidth]{content/images/screens/screenB1}
  \caption{Form submitted, recording will start}
  \label{fig:webApp1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.7\linewidth]{content/images/screens/screenB2}
  \caption{Recording started}
  \label{fig:webApp2}
\end{subfigure}
\caption{The web application recording screens}
\label{fig:webApp}
\end{figure}

Gathering the signals generated by the smartphone's \gls{IMU} sensors was made possible via the web application by using the newly (then) adopted by mobile browsers vendors JavaScript \gls{API}s, DeviceMotion\footnote{DeviceMotionEvent, accessed 01/10/2017 at: \url{https://developer.apple.com/documentation/webkitjs/devicemotionevent}} and DeviceOrientation\footnote{DeviceOrientationEvent, accessed 01/10/2017 at: \url{https://developer.apple.com/documentation/webkitjs/deviceorientationevent}} interfaces. These are Document object model (\gls{DOM})\footnote{JavaScript HTML DOM, accessed 01/10/2017 at: \url{https://www.w3schools.com/js/js_htmldom.asp}} interfaces, called via the \codeword{window}\footnote{The window object, accessed 01/10/2017 at: \url{https://www.w3schools.com/jsref/obj_window.asp}} object. To capture DeviceMotion events the \codeword{window.onDeviceMotion} event handler\footnote{ondevicemotion, accessed 01/10/2017 at: \url{https://developer.apple.com/documentation/webkitjs/domwindow/1632048-ondevicemotion}} was used, whereas to capture DeviceOrientation events the \codeword{window.onDeviceOrientation} event handler\footnote{ondeviceorientation, accessed 01/10/2017 at: \url{https://developer.apple.com/documentation/webkitjs/domwindow/1628872-ondeviceorientation}} was used. 

The DeviceMotion interface supplied information on the acceleration of the device. Through the \codeword{event.acceleration} and \codeword{event.accelerationIncludingGravity} properties\footnote{acceleration, accessed 01/10/2017 at: \url{https://developer.apple.com/documentation/webkitjs/devicemotionevent/1629483-acceleration}} the acceleration of the device in three planes, $x$, $y$, $z$ (Figure \ref{fig:iphoneAxes}) was recorded in $m/s^{2}$. The maximum rate at which the interface would sample the accelerometer was a read-only property of the interface set at 20Hz.

\begin{figure}[h]
	\centering
	\includegraphics[width=.7\linewidth]{content/images/phones/iPhone_device_axes}
	\caption{iPhone 4 \& 4S accelerometer axes (source: \url{https://developer.apple.com/documentation/uikit/uiacceleration})}
	\label{fig:iphoneAxes}
\end{figure}

The DeviceOrientation interface supplied information on the physical orientation of the device. Through the \codeword{event.alpha}\footnote{alpha, accessed 01/10/2017 at: \url{https://developer.apple.com/documentation/webkitjs/deviceorientationevent/1629200-alpha}}, \codeword{event.beta}\footnote{beta, accessed 01/10/2017 at: \url{https://developer.apple.com/documentation/webkitjs/deviceorientationevent/1631254-beta}} and \codeword{event.gamma}\footnote{gamma, accessed 01/10/2017 at: \url{https://developer.apple.com/documentation/webkitjs/deviceorientationevent/1631656-gamma}} properties the angles of rotation of the device were recorded in degrees. The sampling rate of the DeviceOrientation was a read-only property and, similarly to DeviceMotion, would max out at 20Hz. 

A very important metric provided by the combination of accelerometer and gyroscope of the device and made available as a property of the DeviceMotion event was the rotation rate, recorded through \codeword{event.rotationrate.alpha}, \codeword{event.rotationrate.beta} and \codeword{event.rotationrate.gamma} in degrees per second ($^{o}/sec$). The axes of the rotation rate were defined as alpha (yaw) for the rotation around the $z$-axis, beta (roll) for the rotation around the $x$-axis and gamma (pitch) for the rotation of the device around its $y$-axis (Figure \ref{fig:iphoneAxesGyro}). 

\begin{figure}[h]
	\centering
	\includegraphics[width=.7\linewidth]{content/images/phones/iPhone_device_axes_gyro}
	\caption{iPhone 4 \& 4S gyroscope axes (adapted from: \url{https://developer.apple.com/library/content/documentation/UserExperience/Conceptual/DesigningExpandedAdUnits/FeaturesforExpandedAdUnits/FeaturesforExpandedAdUnits.html})}
	\label{fig:iphoneAxesGyro}
\end{figure}

The web application, was intended to be loaded by a device incorporating accelerometer and gyroscope. We used an iPhone 4 because it was one of the best-equipped smartphones at the time, featuring all necessary hardware and supporting the latest JavaScript specifications, as described earlier. However, the code running behind the web application was written with conditional clauses that would allow it to execute on devices lacking a gyroscope sensor, which was common at the time. When we developed the application only Apple's iOS supported the necessary \gls{API}s, but Google had recently announced that its Android operating system would also support these features in later releases. A few months later the Android version 4.4 (Kit Kat)\footnote{KitKat 4.4, accessed 02/10/2017 at: \url{https://www.android.com/versions/kit-kat-4-4/}} added the DeviceMotion and DeviceOrientation interfaces to its web browser, making every Android phone with the required sensors a suitable platform for our implementation ``out of the box''. The web application developed for \gls{SmartCT1}, apart from the JavaScript \gls{API}s gathering the accelerometer and gyroscope signal, consisted of a PHP module which created a session to hold each recording's information, and posted the values of the acceleration, orientation and rotation rate to the Apache server. The Apache server was a simple web server installed in a machine located at the University of Macedonia, operating behind a firewall. 

Functionality-wise, the clinical trial associate or the support staff would mount the iPhone on a participant's hand and load the web application. They would then fill in the necessary information in the step-by-step appearing form elements, have the participant assume the required position for the recording, and submit the form. After a delay of 3 seconds, to allow the user to feel comfortable in the position, the iPhone would start recording data via the JavaScript interfaces DeviceMotion and DeviceOrientation. After 12 seconds the web application would post acceleration and rotation data in a text file to the server, where they would be stored for post-processing. The name of the file would be a combination of the data filled in the form and a timestamp, rendering each file uniquely identifiable in the server. After persisting the file, the web application would immediately redirect to the landing page containing the form, this time already filled via the PHP session with the information of the previous recording. The staff could either choose to repeat the same recording, or put the glove in the participant's opposite hand. The payload of the web application was about 10KB and each file to be uploaded was typically less than 40KB. The web application used no local storage on the smartphone. 

Upon completion of the recordings of \gls{SmartCT1}, the files collected were all downloaded from the server to a personal computer in the University of Macedonia running MATLAB. The data in the personal computer were anonymized and the only trail of identification could be followed using the clinical trial folders containing participants' demographics, disease progression information and the unique identifier used in the web application's initial form. The separation of data and anonymity of the signals were maintained for confidentiality and security reasons. We were handling information in the context of a ``need-to-know'' basis, minimizing the chances of sensitive information being exposed. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Data Collected}
\label{subsec:SmartCT1Data}
During each session of \gls{SmartCT1}, participants were asked to wear the glove-case securing the iPhone on their wrist, and follow the item 3.15 protocol defined in the \gls{MDS}-\gls{UPDRS}, i.e., stretch their arms out in front of them, keeping their palms facing the ground, the wrists straight and the fingers separated, in a comfortable manner. That position was maintained for 12 seconds while data were recorded. The device was mounted on both their hands alternately, and each test was repeated at least twice for each participant. From the data obtained, we formed the participant's acceleration vector, $\alpha(i) = [\alpha_{x}(i),\alpha_{y}(i),\alpha_{z}(i)]^{T}$ (in $m/s^{2}$) and angular velocity vector $\omega(i) = [\omega_{x}(i),\omega_{y}(i),\omega_{z}(i)]^{T}$ (in $rad/s$), with $i$ denoting discrete sample and axes $x$, $y$ and $z$ defined as in figures \ref{fig:iphoneAxes} and \ref{fig:iphoneAxesGyro}. Of the 12 seconds recorded at each session the first 2 seconds were discarded. For 10 seconds signals, at 20Hz sampling rate, each session would yield 200 samples for each axis. As a first signal characteristic we calculated both for the acceleration and the rotation rate, the squared magnitude for each sample as shown in \ref{eq: magSampleAlpha} and \ref{eq: magSampleOmega}. Typical profiles of the two vectors (magnitude squared) are shown in figure \ref{fig:signals}.

\begin{equation} \label{eq: magSampleAlpha}
\|\alpha\|^{2} = \alpha_{x}^{2} + \alpha_{y}^{2} + \alpha_{z}^{2}
\end{equation}

\begin{equation} \label{eq: magSampleOmega}
\|\omega\|^{2} = \omega_{x}^{2} + \omega_{y}^{2} + \omega_{z}^{2}
\end{equation}

\begin{figure}[h]
\centering
\begin{subfigure}{1\textwidth}
  \centering
  \includegraphics[width=0.85\linewidth]{content/images/EMBC11/fig3a}
  \caption{Acceleration (magnitude squared) for a control volunteer compared to a \gls{PD} patient}
  \label{fig:accelSig}
\end{subfigure}

\begin{subfigure}{1\textwidth}
  \centering
  \includegraphics[width=0.85\linewidth]{content/images/EMBC11/fig3b}
  \caption{Rotation rate (magnitude squared) for a control volunteer compared to a \gls{PD} patient}
  \label{fig:rotSig}
\end{subfigure}
\caption{Acceleration and Rotation rate signal sample squared magnitudes}
\label{fig:signals}
\end{figure}

One challenge when using the DeviceMotion and DeviceOrientation JavaScript interfaces when \gls{SmartCT1} was conducted was that the sampling rate was not adjustable, and was set at a default 20Hz, which was quite low. Furthermore, sampling of the on-board devices via JavaScript was subject to jitter (in some cases we measured a jitter of up to 5ms with a nominal sampling period of 50ms). In light of these considerations, we chose to explore very simple signal metrics (e.g., energy or power) for categorizing subjects as to whether they exhibited hand tremor or not. Specifically, we computed

\begin{equation} \label{eq: magAlpha}
m_{\alpha} = \frac{\sum_{i=1}^{N} \|\alpha(i)\|^{2} }{N}
\end{equation}

\begin{equation} \label{eq: magOmega}
m_{\omega} = \frac{\sum_{i=1}^{N} \|\omega(i)\|^{2} }{N}
\end{equation}

\noindent 
where $N$ was the number of samples in the signal (i.e. 200), and attempted to categorize subjects based on these two quantities. Here $m_{\alpha}$ and $m_{\omega}$ can be viewed as signal power, the signal being the length of the acceleration and rotation rate vectors, respectively. Typical values of these metrics when the iPhone was at rest placed stationary on a desk were $m_{\alpha}=0.0127$ and  $m_{\omega}=0.0000$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Data Analysis}
\label{subsec:SmartCT1Analysis}
\subsubsection{Initial Comparison}
\label{subsubsec:SmartCT1InitComp}
Mann-whitney non-parametric U test was used to explore the statistical significance of the difference between the $m_{\alpha}$ and $m_{\omega}$ values of the two populations. 
We first compared the mean values of $m_{\alpha}$ for our two populations, using all data (left and right hand) for each subject. Volunteers performed 2 sessions for each hand, resulting in 4 data signals. Initially, each volunteer was assigned a single number calculated as the average value of $m_{\alpha}$ for all 4 data signals collected. The \gls{SmartH1} population had an average acceleration power $m_{\alpha}$ = 0.0915 (min = 0.0338, max = 0.1325), whereas the \gls{SmartPD1} had an average acceleration power $m_{\alpha}$ = 0.1760 (min = 0.0511, max = 0.6905). Although the max value of the \gls{SmartPD1} population is quite high and the mean values are different, the min values of the two populations suggest that there can be an overlap. This is obvious in the boxplot as well (figure \ref{fig:boxAllA}). As expected, the Mann-Whitney U test could not reject the null hypothesis that the distributions of the values of the two populations are not different, and  confirmed that a randomly selected \gls{SmartPD1} value could equally likely be greater or smaller than a randomly selected \gls{SmartH1} value. 

\begin{figure}[h]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.9\linewidth]{content/images/analysis/boxAllA}
  \caption{$m_{\alpha}$ distribution comparison}
  \label{fig:boxAllA}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.9\linewidth]{content/images/analysis/boxAllOm}
  \caption{$m_{\omega}$ distribution comparison}
  \label{fig:boxAllOm}
\end{subfigure}
\caption{Distributions comparison using volunteers' both hands' mean values}
\label{fig:boxAll}
\end{figure}

\begin{table}[h]
\centering
\caption{\textsc{SmartH1 Mean Scores (Both Hands)}}
\begin{tabular*}{1\textwidth}{@{\extracolsep{\fill}} c  c  c}
 	\textit{Volunteers} & $m_{\alpha}$(\textit{Acceleration}) & $m_{\omega}$(\textit{Rotation Rate})\\
	\hline 	\hline 		
 	Control Mean Value & 0.0915 & 0.0039 \\ 
 	\hline
 	\gls{SmartH1}$_{1}$ & 0.0348 & 0.0046\\ 
 	\gls{SmartH1}$_{2}$ & 0.1235 & 0.0032\\ 
 	\gls{SmartH1}$_{3}$ & 0.1025 & 0.0066\\ 
 	\gls{SmartH1}$_{4}$ & 0.1325 & 0.0062\\ 
 	\gls{SmartH1}$_{5}$ & 0.1258 & 0.0025\\ 
 	\gls{SmartH1}$_{6}$ & 0.0615 & 0.0020\\
 	\gls{SmartH1}$_{7}$ & 0.1064 & 0.0027\\
 	\gls{SmartH1}$_{8}$ & 0.0595 & 0.0039\\
 	\gls{SmartH1}$_{9}$ & 0.1139 & 0.0035\\
 	\gls{SmartH1}$_{10}$ & 0.0556 & 0.0037\\
 	 &  & \\
	\multicolumn{3}{c}{Numbers represent the volunteers' mean scores, } 	\\
	\multicolumn{3}{c}{using measurements from both hands} \\
\end{tabular*}
\label{table:allSmartH1}
\end{table}

\begin{table}[!hbp]
\centering
\caption{\textsc{SmartPD1 Mean Scores (Both Hands)}}
\begin{tabular*}{1\textwidth}{@{\extracolsep{\fill}} c  c  c}
 	\textit{Volunteers} & $m_{\alpha}$(\textit{Acceleration}) & $m_{\omega}$(\textit{Rotation Rate})\\
	\hline 	\hline 		
 	Control Mean Value & 0.0915 & 0.0039 \\ 
 	\hline
 	Control Max Value & 0.1325 & 0.0066 \\ 
 	\hline
 	\gls{SmartPD1}$_{1}$ & 0.0578 & 	0.0161 \\
 	\gls{SmartPD1}$_{2}$ & 0.0582 & 	0.0115 \\
 	\gls{SmartPD1}$_{3}$ & 0.3636 & 	0.0510 \\
 	\gls{SmartPD1}$_{4}$ & 0.0742 & 	0.0326 \\
 	\gls{SmartPD1}$_{5}$ & 0.1494 & 	0.0103 \\
 	\gls{SmartPD1}$_{6}$ & 0.0532 & 	0.0168 \\
 	\gls{SmartPD1}$_{7}$ & 0.0866 & 	0.0090 \\
 	\gls{SmartPD1}$_{8}$ & 0.0511 & 	0.0041 \\
 	\gls{SmartPD1}$_{9}$ & 0.6905 & 	0.0503 \\
 	\gls{SmartPD1}$_{10}$ & 72.3708 &	1.5445 \\
 	 &  & \\
	\multicolumn{3}{c}{Numbers represent the volunteers' mean scores, } 	\\
	\multicolumn{3}{c}{using measurements from both hands} \\
\end{tabular*}
\label{table:allSmartPD1}
\end{table}

With respect to rotation rate power, the \gls{SmartH1} population had a mean of 0.0039 (min = 0.0020, max = 0.0066), whereas the average for \gls{SmartPD1} subjects was a much higher 0.0224 (min = 0.0041, max = 0.0510). In this case, the Mann-Whitney U test rejected the null hypothesis of same distributions, with a p-value nearly equal to zero, confirming that a randomly selected value from the \gls{SmartPD1} population would most likely be larger than a randomly selected value from the control population. The boxplot confirms visually that the two distributions are different (figure \ref{fig:boxAllOm}). 

In both cases, the 10th \gls{SmartPD1} subject's scores were much higher than the rest, and were excluded when calculating the mean for the \gls{SmartPD1} group, in order to avoid ``artificially'' strong comparisons with the \gls{SmartH1} group.

All values calculated as means from both hands for each control volunteer and each patient are shown in tables \ref{table:allSmartH1} and \ref{table:allSmartPD1}, respectively. 

\subsubsection{Digging Deeper}
\label{subsubsec:SmartCT1DigDeep}
As depicted in figure \ref{fig:bars1}, all \gls{SmartPD1} subjects scored higher than the control mean when considering the $m_{\omega}$ metric (in 4 cases they scored higher in both metrics), and 9 out of 10 scored higher than the higher \gls{SmartH1} score (0.0066). 

\begin{figure}[!htp]
	\centering
	\includegraphics[width=1\linewidth]{content/images/EMBC11/fig4}
	\caption{Bars represent the volunteers' mean scores using measurements from both hands. For the $m_{\omega}$ metric the scores are scaled by 10.}
	\label{fig:bars1}
\end{figure}

\begin{table}[h]
\centering
\caption{\textsc{SmartH1 Mean Scores (Worst Hand)}}
\begin{tabular*}{1\textwidth}{@{\extracolsep{\fill}} c  c  c}
 	\textit{Volunteers} & $m_{\alpha}$(\textit{Acceleration}) & $m_{\omega}$(\textit{Rotation Rate})\\
	\hline 	\hline 		
 	Control Max Value & 0.1515 & 0.0092 \\ 
 	\hline
 	\gls{SmartH1}$_{1}$ & 0.0432 & 0.0067\\ 
 	\gls{SmartH1}$_{2}$ & 0.1314 & 0.0035\\ 
 	\gls{SmartH1}$_{3}$ & 0.1087 & 0.0092\\ 
 	\gls{SmartH1}$_{4}$ & 0.1412 & 0.0075\\ 
 	\gls{SmartH1}$_{5}$ & 0.1515 & 0.0041\\ 
 	\gls{SmartH1}$_{6}$ & 0.0762 & 0.0030\\
 	\gls{SmartH1}$_{7}$ & 0.1221 & 0.0030\\
 	\gls{SmartH1}$_{8}$ & 0.0687 & 0.0060\\
 	\gls{SmartH1}$_{9}$ & 0.1258 & 0.0057\\
 	\gls{SmartH1}$_{10}$ & 0.0625 & 0.0054\\
 	 &  & \\
	\multicolumn{3}{c}{Numbers represent the volunteers' mean scores, } 	\\
	\multicolumn{3}{c}{using measurements from the worst hand} \\
\end{tabular*}
\label{table:worstSmartH1}
\end{table}

\begin{table}[!hbp]
\centering
\caption{\textsc{SmartPD1 Mean Scores (Worst Hand)}}
\begin{tabular*}{1\textwidth}{@{\extracolsep{\fill}} c  c  c}
 	\textit{Volunteers} & $m_{\alpha}$(\textit{Acceleration}) & $m_{\omega}$(\textit{Rotation Rate})\\
	\hline 	\hline 		
 	Control Max Value & 0.1515 & 0.0092 \\ 
 	\hline
 	\gls{SmartPD1}$_{1}$ & 0.0850 & 	0.0223 \\
 	\gls{SmartPD1}$_{2}$ & 0.0645 & 	0.0177 \\
 	\gls{SmartPD1}$_{3}$ & 0.4684 & 	0.0662 \\
 	\gls{SmartPD1}$_{4}$ & 0.1116 & 	0.0335 \\
 	\gls{SmartPD1}$_{5}$ & 0.2371 & 	0.0178 \\
 	\gls{SmartPD1}$_{6}$ & 0.0545 & 	0.0307 \\
 	\gls{SmartPD1}$_{7}$ & 0.1014 & 	0.0148 \\
 	\gls{SmartPD1}$_{8}$ & 0.0570 & 	0.0060 \\
 	\gls{SmartPD1}$_{9}$ & 1.1056 &  0.0746\\
 	\gls{SmartPD1}$_{10}$ & 144.6557 &	3.0720 \\
 	 &  & \\
	\multicolumn{3}{c}{Numbers represent the volunteers' mean scores, } 	\\
	\multicolumn{3}{c}{using measurements from both hands} \\
\end{tabular*}
\label{table:worstSmartPD1}
\end{table}

The acceleration and rotation rate metrics we computed, off-the-record, generally agreed with the severity of hand tremor, as observed clinically. One of our goals was to see how well we can detect movement disorders (in this case, parkinsonian hand tremor) using $m_{\alpha}$ and $m_{\omega}$. Towards that end, we explored the possibility of identifying subjects with hand tremor via two simple criteria. The first was whether either of a subject's mean measurements ($m_{\alpha}$, $m_{\omega}$) were above a certain threshold, namely the highest mean score encountered in the \gls{SmartH1} group across the trial. Because it is known that \gls{PD} typically affects one side of the patient more severely than the other (see section \ref{subsec:laterality} on laterality), we re-calculated each metric's mean value using data only from each subject's ``worst-performing'' hand, i.e., the hand which scored highest in each session (tables \ref{table:worstSmartH1} and \ref{table:worstSmartPD1}). By doing so, the highest acceleration and rotation rate scores for healthy subjects were $SmartH1m_{\alpha}Max=0.1515$ and $SmartH1m_{\omega}Max=0.0092$, respectively.  The comparisons between the subjects' mean scores for their worst-performing hand and the thresholds (maximum scores) obtained from the control group are described in figure \ref{fig:bars2}. The thresholding criterion correctly ``labels'' nine of the ten subjects as being in the \gls{SmartPD1} group. The one exception was \gls{PD} patient number 8 who's symptoms include mostly bradykinesia and rigidity, and not pronounced hand tremor.

Similarly to working with all metrics' scores, Mann-Whtney U testing rejected the hypothesis that $m_{\omega}$ \gls{SmartH1} worst-hand scores and \gls{SmartPD1} worst-hand scores belong to the same distribution, calculating significantly different means. Contrary, isolating worst-hand performances did not affect the $m_{\alpha}$ scores, which regardless of population would probably belong to the same distribution, rendering their comparison insignificant. Means testing results were confirmed because if we were to judge based on  $m_{\omega}$ alone, then the same nine subjects as before (patients above the control mean in figure \ref{fig:bars1}) would be true positives, whereas only four subjects exceeded the $m_{\alpha}$ acceleration threshold, proving the $m_{\omega}$ to be a stronger separation criterion than the $m_{\alpha}$ alone.

\begin{figure}[!htp]
	\centering
	\includegraphics[width=1\linewidth]{content/images/EMBC11/fig5}
	\caption{Bars represent the volunteers' mean scores using measurements only from worst-performing (highest-scoring) hand. For the $m_{\omega}$ metric the scores are scaled by 10.}
	\label{fig:bars2}
\end{figure}

We must note that the identification of patients' worst-performing hand by comparing average metrics for their left versus right hand, agreed for both metrics, and coincided with their actual most affected side for every patient except \gls{SmartPD1} volunteer number 4, yielding a 90\% accuracy in laterality identification.

A second criterion for comparing the \gls{SmartPD1} and \gls{SmartH1} populations was the difference between hands for both $m_{\alpha}$ and $m_{\omega}$. A healthy volunteer would not be expected to have very different left versus right mean scores, whereas, due to laterality (see section \ref{subsec:laterality}), the opposite should be true for patients with parkinsonism. Regarding $m_{\alpha}$, the control group had an average hand-to-hand difference of 0.0232 (max = 0.0514, min = 0.0123), whereas \gls{SmartPD1} participants had an average difference of 0.1572. The corresponding numbers for $m_{\omega}$ were 0.0031 (max = 0.0053, min = 0.0006) for the \gls{SmartH1} group and 0.0185 for the \gls{SmartPD1} group. If we consider the \gls{SmartH1} maximum difference between hands as a threshold, i.e., $SmartH1m_{\alpha}DifMax = 0.0514$ and $SmartH1m_{\omega}DifMax = 0.0053$, nine of the patients had mean differences which exceeded the maximum encountered in the healthy group in either $m_{\alpha}$, $m_{\omega}$, or both; they were the same nine identified via thresholding on worst-hand scores. The comparison of the difference in hands' scores for the two populations makes sense at a 5\% significance level, given that Mann-Whitney U testing revealed a significant means difference with $p = 0.0155$, only for the $m_{\omega}$ metric and same distributions for the $m_{\alpha}$ metric. As before, we excluded the 10th patient to avoid misinterpreting the results. In general, acceleration alone could not be trusted as a classification criteria at such low sampling rate i.e., 20Hz and using only signal power as a feature. The values of the differences between hands are shown in tables \ref{table:difSmartH1} and \ref{table:difSmartPD1} for \gls{SmartH1} and \gls{SmartPD1}, respectively. 

\begin{table}[!htp]
\centering
\caption{\textsc{SmartH1 Differences Between Hands}}
\begin{tabular*}{1\textwidth}{@{\extracolsep{\fill}} c  c  c}
 	\textit{Volunteers} & $m_{\alpha}$(\textit{Acceleration}) & $m_{\omega}$(\textit{Rotation Rate})\\
	\hline 	\hline 		
 	Control Max Value & 0.0514 & 0.0053 \\ 
 	\hline
 	\gls{SmartH1}$_{1}$ & 0.0188 & 0.0042\\ 
 	\gls{SmartH1}$_{2}$ & 0.0158 & 0.0006\\ 
 	\gls{SmartH1}$_{3}$ & 0.0123 & 0.0053\\ 
 	\gls{SmartH1}$_{4}$ & 0.0174 & 0.0027\\ 
 	\gls{SmartH1}$_{5}$ & 0.0514 & 0.0031\\ 
 	\gls{SmartH1}$_{6}$ & 0.0293 & 0.0020\\
 	\gls{SmartH1}$_{7}$ & 0.0313 & 0.0007\\
 	\gls{SmartH1}$_{8}$ & 0.0184 & 0.0043\\
 	\gls{SmartH1}$_{9}$ & 0.0239 & 0.0044\\
 	\gls{SmartH1}$_{10}$ & 0.0138 & 0.0034\\
 	 &  & \\
	\multicolumn{3}{c}{Numbers represent the volunteers' mean differences between hands} 	\\
\end{tabular*}
\label{table:difSmartH1}
\end{table}

\begin{table}[!htp]
\centering
\caption{\textsc{SmartPD1 Differences Between Hands}}
\begin{tabular*}{1\textwidth}{@{\extracolsep{\fill}} c  c  c}
 	\textit{Volunteers} & $m_{\alpha}$(\textit{Acceleration}) & $m_{\omega}$(\textit{Rotation Rate})\\
	\hline 	\hline 		
 	Control Max Value & 0.0514 & 0.0053 \\ 
 	\hline
 	\gls{SmartPD1}$_{1}$ & 0.0680 & 	0.0154 \\
 	\gls{SmartPD1}$_{2}$ & 0.0125 & 	0.0125 \\
 	\gls{SmartPD1}$_{3}$ & 0.2096 & 	0.0303 \\
 	\gls{SmartPD1}$_{4}$ & 0.0748 & 	0.0017 \\
 	\gls{SmartPD1}$_{5}$ & 0.1754 & 	0.0150 \\
 	\gls{SmartPD1}$_{6}$ & 0.0027 & 	0.0277 \\
 	\gls{SmartPD1}$_{7}$ & 0.0296 & 	0.0115 \\
 	\gls{SmartPD1}$_{8}$ & 0.0116 & 	0.0038 \\
 	\gls{SmartPD1}$_{9}$ & 0.8303 &  0.0485\\
 	\gls{SmartPD1}$_{10}$ & 144.5698 &	3.0551 \\
 	 &  & \\
	\multicolumn{3}{c}{Numbers represent the volunteers' mean differences between hands} 	\\
\end{tabular*}
\label{table:difSmartPD1}
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Discussion}
\label{subsec:SmartCT1Discussion}
The purpose of this first study was to collect data by integrating a smartphone, as seamlessly as possible, into the clinical assessment of item 3.15 of the gold standard of clinical evaluation, i.e., \gls{UPDRS}, which is used to assess postural tremor of the hands, and to explore whether these data could have value in quantifying and identifying \gls{PD} related tremor. 

We explored the use of recently-available smartphone technology for the purpose of diagnosing and quantifying the postural tremor which is one of the cardinal symptoms of parkinsonism. Our method was based on the use of the accelerometer and gyroscope currently embedded in many smartphone models and accessible via web development technologies, i.e., JavaScript. 
With \gls{SmartCT1} we particularly aimed at exploring the robustness of a tool that would be:

\begin{enumerate}
\item easy to handle, not requiring intricate installation steps or specific resources to run, 
\item readily available, within reach for most people, requiring zero expenses to purchase,
\item unobtrusive in its operation, able to be used complementary to current clinical assessment protocol and
\item accurate, comparable in performance with clinical mainstay.
\end{enumerate}

The first three points were fulfilled. The web application was extremely easy to operate and none of the participants, control or patients, expressed any concern regarding the wearability of the custom-made glove-case. The duration of the assessment did not cause any discomfort and the fact that this tool could be used with no time or space constraints was appreciated by the physicians to whom it was presented. 

Regarding the final point, the tool's performance, the clinical trial proved that even the simplest processing of the signals collected could reveal usable information. Within \gls{SmartCT1} we could correctly categorize nine of the ten patients, based mainly on the combination of their hands' acceleration and angular position, while assuming the \gls{MDS}-\gls{UPDRS} item 3.15 assessment position. The gyroscope data proved to be particularly useful in that regard, capturing the rotational component of the upper limb tremor profile. 

\gls{PD} induced resting and postural limb tremor occurs at 4-10Hz (see section \ref{subsec:tremor}). That means that according to the Nyquist-Shannon's sampling theorem, a mechanical way of identifying it would have to support a sampling rate of at least double that, i.e., 20Hz. Ideally, the sampling rate of the accelerometer and gyroscope should be well above 20Hz for us to be certain that \gls{PD}-related tremor is sufficiently recorded. However, sampling rate was a read-only property for the interfaces we used and maxed out at 20Hz. The relatively low sampling frequency of the sensors allowed by the JavaScript interfaces combined with the small duration of the sessions, resulted in Mann-Whitney U testing not identifying any differences in the acceleration power calculated for the two populations. Nevertheless, the rotation rate metric, exploiting the gyroscope sensor proved to be enough to identify pathological postures and capture laterality for the \gls{SmartPD1} group in \gls{SmartCT1}. 


%%%%%%%%%%%%%%%%%%%% END OF SMARTCT1 %%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{SmartCT2}
\label{sec:SmartCT2}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Protocol}
\label{subsec:SmartCT2Protocol}

\subsubsection{Rationale}
\label{subsubsec:smartCT2Rationale}
Two valuable conclusions were drawn from the \gls{SmartCT1} results:

\begin{enumerate}
\item Our web-based tool could be used as a complement to the current clinical mainstay to evaluate tremor.
\item Improvements would have to be made wherever possible to eliminate the shortcomings of the previous protocol and make our solution more robust and valuable as a tremor quantification tool. 
\end{enumerate}

The second conclusion drove us to our second clinical trial, \gls{SmartCT2}. We designed \gls{SmartCT2} having validation in mind. In order to validate our tool it would be necessary to compare it to results from \gls{UPDRS} assessments conducted at the same time of the recording sessions. To that end we designed three separate phases within the trial:

\begin{itemize}
\item In phase I (\gls{SmartCT2PI}) we collected signals from patients with parkinsonism manifestations, which we used to run correlation analysis, using their \gls{UPDRS} scores as input. In order for our tool to be validated as a tremor quantification approach it should prove to perform at least as well as the ``gold standard'' of \gls{PD} clinical examination, the \gls{UPDRS}. The new group of patients, \gls{SmartPD2} was recruited to be assessed both using the \gls{UPDRS} and our web-based smartphone tool. A correlation analysis was conducted to reveal whether the two methods produce similar results. \gls{SmartCT2PI} alone was actually a one population validation study. 
\item In phase II (\gls{SmartCT2PII}) we collected signals from a control group with strict inclusion criteria to pursuit the creation of a classification machine-learning model. The new control group, \gls{SmartH2} was recruited to be used as the alternative class compared to the patients. Both groups' performances calculated with our tool were used to train a supervised machine-learning algorithm to identify the class of new unlabeled signals. 
\item In phase III - longitudinal (\gls{SmartCT2L}) we conducted a small longitudinal study to further evaluate our tool's performance in identifying changes in a single patient's symptom severity over time. This phase's group, \gls{SmartPD2L} participated in recording sessions during both the ON and OFF state (see section \ref{sec:dyskinesia}), to explore our tool's capacity in identifying same-patient fluctuations in tremor.
\end{itemize}

\subsubsection{Volunteers}
\label{subsubsec:smartCT2Volunteers}
To conduct \gls{SmartCT2PI} we recruited 23 \gls{PD} patients, \gls{SmartPD2}, from the outpatient clinic of the 1st Department of Neurology of AHEPA University General Hospital, located in the territory of the Aristotle University of Thessaloniki. All subjects agreed to participate in this research after a detailed explanation of its aims and of the testing procedure. All \gls{SmartPD2} volunteers were right-handed, under L-DOPA treatment and suffering from \gls{PD} for more than two years.
For the \gls{SmartCT2L} phase we recruited two \gls{PD} patients, \gls{SmartPD2L} who were hospitalized overnight in the AHEPA University General Hospital, so that they could be recorded in the morning before they received their medication, aiming to capture their OFF state. Table \ref{table:demoSmartPD2} summarizes information on the patients participating in \gls{SmartCT2}. 

\begin{table}[!hp]
\centering
\caption{\textsc{Information on SmartPD2 and SmartPD2L groups}}
\begin{tabular*}{1\textwidth}{@{\extracolsep{\fill}} c c c c c c c}
	\multirow{2}{*}{\textit{Volunteers}} & \multirow{2}{*}{Age} & \multirow{2}{*}{Sex} 
	& \multicolumn{4}{c}{\textit{UPDRS Upper Limb Tremor Items}} \\
	\cline{4-7}
	& & & \textit{Rest Right} & \textit{Rest Left} & \textit{Extended Right} & \textit{Extended Left} \\
	\hline 	\hline 
 	\gls{SmartPD2}$_{1}$ & 76 & F & 0 & 1 & 0 & 1 \\
 	\gls{SmartPD2}$_{2}$ & 77 & M & 0 & 0 & 0 & 1 \\
 	\gls{SmartPD2}$_{3}$ & 80 & F & 2 & 0 & 0 & 0 \\
 	\gls{SmartPD2}$_{4}$ & 76 & M & 1 & 0 & 1 & 0 \\
 	\gls{SmartPD2}$_{5}$ & 74 & M & 0 & 0 & 0 & 1 \\
 	\gls{SmartPD2}$_{6}$ & 76 & M & 0 & 2 & 0 & 0 \\
 	\gls{SmartPD2}$_{7}$ & 62 & F & 0 & 1 & 1 & 1 \\
 	\gls{SmartPD2}$_{8}$ & 82 & M & 0 & 0 & 1 & 1 \\
 	\gls{SmartPD2}$_{9}$ & 69 & F & 0 & 0 & 0 & 1 \\
 	\gls{SmartPD2}$_{10}$ & 76 & F & 1 & 2 & 1 & 2 \\
 	\gls{SmartPD2}$_{11}$ & 81 & M & 1 & 0 & 2 & 1 \\
 	\gls{SmartPD2}$_{12}$ & 39 & M & 2 & 1 & 2 & 2 \\
 	\gls{SmartPD2}$_{13}$ & 65 & F & 0 & 2 & 0 & 1 \\
 	\gls{SmartPD2}$_{14}$ & 78 & M & 2 & 0 & 1 & 1\\
 	\gls{SmartPD2}$_{15}$ & 50 & M & 1 & 0 & 0 & 0\\
 	\gls{SmartPD2}$_{16}$ & 75 & F & 0 & 0 & 0 & 1\\
 	\gls{SmartPD2}$_{17}$ & 70 & M & 0 & 0 & 1 & 2 \\
 	\gls{SmartPD2}$_{18}$ & 80 & F & 4 & 4 & 2 & 1 \\
 	\gls{SmartPD2}$_{19}$ & 76 & M & 0 & 0 & 0 & 0 \\
 	\gls{SmartPD2}$_{20}$ & 75 & F & 2 & 0 & 2 & 1 \\
 	\gls{SmartPD2}$_{21}$ & 43 & F & 0 & 0 & 0 & 0 \\
 	\gls{SmartPD2}$_{22}$ & 78 & F & 1 & 0 & 2 & 1 \\
 	\gls{SmartPD2}$_{23}$ & 73 & F & 0 & 0 & 0 & 0 \\
 	& & & & & & \\
 	\hdashline
 	\gls{SmartPD2L}$_{1}$ & 83 & M & 1 & 0 & 0 & 0 \\
 	\gls{SmartPD2L}$_{2}$ & 77 & F & 0 & 1 & 2 & 1 \\
 	& & & & & & \\
	\multicolumn{7}{c}{\gls{UPDRS} scores evaluated in ON state} 	\\
\end{tabular*}
\label{table:demoSmartPD2}
\end{table}

To complete \gls{SmartCT2PI} we recruited 20 healthy volunteers, \gls{SmartH2}. They were screened for several health conditions which could exclude them from the study, such as hypertension or any movement disorder. They were also notified of the procedure and the purpose of the study before agreeing to participate. Grouping information on all participants of the study is provided in table \ref{table:demoSmartCT2}. 

The volunteers' ages for all the groups were mean-tested with the non-parametric Mann-Whitney U test and were found not to be statistically different at the 1\% significance level, therefore the groups could be considered age-matched.

\begin{table}[!hp]
\centering
\caption{\textsc{Information on Volunteers' Grouping}}
\begin{tabular*}{1\textwidth}{@{\extracolsep{\fill}} c c c c c c }
	\multirow{2}{*}{\textit{Group}} & \multirow{2}{*}{Size}
	& \multicolumn{2}{c}{\textit{Sex Statistics}} & \multicolumn{2}{c}{\textit{Age Statistics}} \\
	\cline{3-6}
	& & \textit{Females} & \textit{Males} & \textit{Mean$\pm$StD} & \textit{SE} \\
	\hline 	\hline 
	\textit{SmartH2} & 20 & 10 & 10 & 67.20$\pm$6.25 & 1.39 \\
	\textit{SmartPD2} & 23 & 12 & 11 & 70.91$\pm$11.78 & 2.45 \\
	\textit{SmartPD2L} & 20 & 10 & 10 & 80.00$\pm$4.24 & 3.00 \\
 	\hline
 	\textit{Total} & 45 & 23 & 22 & & \\
\end{tabular*}
\label{table:demoSmartCT2}
\end{table}

\subsubsection{Procedure and Hardware}
\label{subsubsec:smartCT2ProcHardware} 
The regimen that had to be followed during all phases of both smartphone-based clinical trials was similar, albeit a few minor modifications after the first trial. As already discussed, the first thing we changed was the size of the groups and distinguishing different phases for each desired output. The hardware used was exactly the same as the one used in \gls{SmartCT1} (see section \ref{subsubsec:smartCT1ProcHardware}). All volunteers had to wear the custom-made glove-case with an iPhone mounted securely on the dorsal part of their hand. However, for all phases of \gls{SmartCT2} we made the following two very important changes:

\begin{enumerate}
\item We added a new posture to assess resting tremor as well as action postural tremor. Since we wanted our metrics and results from \gls{SmartCT2} to be compared with the clinical examination mainstay, the \gls{UPDRS}, we applied the same instructions regarding body posture as described in item 3.15 - Postural tremor of the hands, like we did for \gls{SmartCT1}, and  3.17 - Rest tremor amplitude of the \gls{MDS}-\gls{UPDRS} (Goetz et al, 2008). More specifically, in order to assess postural hand tremor we instructed the volunteers to stretch their arms out in front of the body, keeping their palms facing the ground. The wrists were to be held straight and the fingers separated as to not touch each other, in a comfortable manner. To assess rest tremor we instructed the volunteers to sit comfortably in a chair, with their arms placed on the arms of the chair, without touching their lap, and their feet loosely placed on the floor.
\item We increased the duration of the two postures used to 30 seconds. Unfortunately there was no way to increase the sampling rate of the smartphone's sensors using the DeviceMotion and DeviceOrientation JavaScript interfaces (see section \ref{subsubsec:smartCT1Software}, which was still limited at the marginally acceptable 20Hz threshold. To increase our potential of retrieving valuable information in the collected signals we decided to increase their duration. Increasing the duration of each individual recording, would still keep the total session duration at 240 seconds, i.e., 4 minutes, given that for both postures the device was mounted on both the volunteers' hands alternately, and each recording was repeated twice, resulting in 8 recordings of 30 seconds each.
\end{enumerate}

\noindent
The 23 volunteers of the \gls{SmartPD2} group who underwent the smartphone-based tremor measuring procedure were under medication, but the timespan from their last dose of L-DOPA was anywhere from 1 to 4 hours when they were tested. That means that some of them were at the peak of the drug’s effect while for others this was not the case. For the third phase of the study, \gls{SmartCT2L}, we wanted to see how our tool would perform against an alteration in a patient's condition, such as that brought on by medication intake. The two \gls{SmartPD2L} volunteers stayed in the clinic overnight and followed our experimental protocol both OFF and ON medication, i.e., right before taking their medication in the morning and one hour after that. 

Throughout \gls{SmartCT2}, every time a patient participated in a session, an experienced neurologist would perform a full \gls{UPDRS} examination before the recording. The neurologist was the same for every patient to avoid inter-rater variability. 

As per the \gls{SmartCT1} protocol described earlier, all volunteers were asked to maintain the postures for the defined duration, in the case of \gls{SmartCT2}, 30 seconds, during which the web application running on the glove-case-mounted iPhone automatically collected the accelerometer and gyroscope data and sent them to our server.
 
\subsubsection{Software}
\label{subsubsec:smartCT2Software}
The software modules used in all phases of \gls{SmartCT2} were to a great extent the same as those used in \gls{SmartCT1}. Apart from collecting signals for a longer period of time, the web application was the same, the Apache server was identical, as was the pipeline executed up to the post processing of the signals. The difference was in the data processing, once the signals were collected, as will be discussed in the following section. When our second trial took place (late 2013 to mid 2014) we actually tested our web application on two Android devices\footnote{a Samsung Galaxy S4 and a Google Nexus 5, both running Android 4.4.2} and it would run successfully. 

%%------------------------------------------------------------------------------------>>>>>>>>>>>>>>>>>>>>>>>>>>>
%%------------------------------------------------------------------------------------>>>>>>>>>>>>>>>>>>>>>>>>>>>
%%------------------------------------------------------------------------------------>>>>>>>>>>>>>>>>>>>>>>>>>>>
%%------------------------------------------------------------------------------------>>>>>>>>>>>>>>>>>>>>>>>>>>>
%%------------------------------------------------------------------------------------>>>>>>>>>>>>>>>>>>>>>>>>>>>
%%------------------------------------------------------------------------------------>>>>>>>>>>>>>>>>>>>>>>>>>>>
%%------------------------------------------------------------------------------------>>>>>>>>>>>>>>>>>>>>>>>>>>>
%%------------------------------------------------------------------------------------>>>>>>>>>>>>>>>>>>>>>>>>>>>

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Data Collected}
\label{subsec:SmartCT2Data}

This paper is a continuation of our previous work [3]; its main contribution is a statistical comparison between signal-based methods of quantifying Parkinsonian tremor using a smartphone, and the UPDRS scores assigned by a physician specialist, in order to validate our previous approach. We acquired hand tremor signals from twenty-three patients using an iPhone and computed the correlation (Pearson product-moment) between the metrics under consideration and the patients’ UPDRS scores regarding hand tremor. Our results indicate a strong correlation (r>0.7) with high statistical significance (p<0.01), which suggests that our quantitative methods of measuring Parkinsonian hand tremor [3] show promise as a means of systematically tracking that component of the disease, possibly as part of a clinical exam or in telemedicine applications. 

In the following, we will specify the combination of a patient’s hand (Right of Left upper extremity) during each position as rR, rL, eR, and eL for rest-right, rest-left, extended-right, and extended-left, respectively.

As per the protocol described earlier, the volunteers were asked to maintain certain postures for 30 seconds, during which the application automatically collected the accelerometer and gyroscope data and sent them to our server. We then used the data to extract features which quantify and characterize the subjects’ tremor levels. The signals were sampled at 20Hz, which is sufficient to identify events occurring at 9Hz or less [20], such as PD-induced tremor. 
Our web application, being written in PHP and JavaScript, is entirely independent of the client's hardware or software platform. It only demands basic prerequisites such as an embedded accelerometer and gyroscope and one of the most popular smartphone operating systems, iOS or Android. 



$\alpha(i) = [\alpha_{x}(i),\alpha_{y}(i),\alpha_{z}(i)]^{T}$
$\omega(i) = [\omega_{x}(i),\omega_{y}(i),\omega_{z}(i)]^{T}$

%Equations

\begin{equation} \label{eq: magAlpha2}
mag_{\alpha} = \sum_{i=1}^{N} \|\alpha(i)\|^{2}
\end{equation}

\begin{equation} \label{eq: magOmega2}
mag_{\omega} = \sum_{i=1}^{N} \|\omega(i)\|^{2}
\end{equation}

\begin{equation} \label{eq: sdAlpha}
sd_{\alpha} = \sum_{i=1}^{N-1}\sum_{\kappa \in \{x,y,z\}} |\alpha_{\kappa}(i) - \alpha_{\kappa}(i+1)|
\end{equation}

\begin{equation} \label{eq: maxAmpOmega}
mAmp_{\omega} = \sum_{\kappa \in \{x,y,z\}} \max_{4 \leq \xi \leq 7} \hat{\omega}_{\kappa}(\xi)
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Data Analysis}
\label{subsec:SmartCT2Analysis}